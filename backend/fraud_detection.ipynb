{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "def generate_synthetic_expense_data(num_records=1000, num_employees=50):\n",
    "    \"\"\"\n",
    "    Generate synthetic expense report data with some intentional anomalies and policy violations.\n",
    "    \"\"\"\n",
    "    # Static data\n",
    "    departments = ['Sales', 'Engineering', 'Marketing', 'HR', 'Finance']\n",
    "    expense_categories = ['Travel', 'Meals', 'Office Supplies', 'Technology', 'Training']\n",
    "    vendors = ['Airlines Corp', 'Hotel Chain', 'Restaurant A', 'Office Store', 'Tech Store', \n",
    "               'Training Corp', 'Local Restaurant', 'Online Retailer']\n",
    "    payment_methods = ['Corporate Card', 'Personal Card', 'Cash']\n",
    "    \n",
    "    # Budget limits per category\n",
    "    category_limits = {\n",
    "        'Travel': 5000,\n",
    "        'Meals': 200,\n",
    "        'Office Supplies': 500,\n",
    "        'Technology': 2000,\n",
    "        'Training': 1500\n",
    "    }\n",
    "    \n",
    "    # Department-specific authorized categories\n",
    "    dept_categories = {\n",
    "        'Sales': ['Travel', 'Meals', 'Technology'],\n",
    "        'Engineering': ['Technology', 'Training', 'Office Supplies'],\n",
    "        'Marketing': ['Travel', 'Meals', 'Technology', 'Training'],\n",
    "        'HR': ['Office Supplies', 'Training'],\n",
    "        'Finance': ['Office Supplies', 'Technology']\n",
    "    }\n",
    "    \n",
    "    # Generate base data\n",
    "    data = []\n",
    "    start_date = datetime(2024, 1, 1)\n",
    "    \n",
    "    for _ in range(num_records):\n",
    "        employee_id = f'EMP{random.randint(1, num_employees):03d}'\n",
    "        department = random.choice(departments)\n",
    "        submission_date = start_date + timedelta(days=random.randint(0, 365))\n",
    "        expense_date = submission_date - timedelta(days=random.randint(0, 30))\n",
    "        \n",
    "        # Intentionally create some policy violations\n",
    "        is_violation = random.random() < 0.15  # 15% chance of violation\n",
    "        \n",
    "        if is_violation:\n",
    "            # Generate a violation case\n",
    "            violation_type = random.choice(['over_budget', 'unauthorized', 'duplicate'])\n",
    "            \n",
    "            if violation_type == 'over_budget':\n",
    "                category = random.choice(list(category_limits.keys()))\n",
    "                amount = category_limits[category] * random.uniform(1.1, 2.0)  # Exceed limit\n",
    "            elif violation_type == 'unauthorized':\n",
    "                category = random.choice(expense_categories)\n",
    "                while category in dept_categories[department]:\n",
    "                    category = random.choice(expense_categories)\n",
    "                amount = random.uniform(50, 500)\n",
    "            else:  # duplicate\n",
    "                category = random.choice(expense_categories)\n",
    "                amount = random.uniform(50, category_limits[category])\n",
    "        else:\n",
    "            # Generate a normal case\n",
    "            category = random.choice(dept_categories[department])\n",
    "            amount = random.uniform(50, category_limits[category] * 0.8)\n",
    "        \n",
    "        record = {\n",
    "            'expense_id': f'EXP{_:06d}',\n",
    "            'employee_id': employee_id,\n",
    "            'department': department,\n",
    "            'submission_date': submission_date.strftime('%Y-%m-%d'),\n",
    "            'expense_date': expense_date.strftime('%Y-%m-%d'),\n",
    "            'amount': round(amount, 2),\n",
    "            'currency': 'USD',\n",
    "            'vendor': random.choice(vendors),\n",
    "            'category': category,\n",
    "            'payment_method': random.choice(payment_methods),\n",
    "            'has_justification': random.random() > 0.1,  # 10% chance of missing justification\n",
    "            'is_violation': is_violation\n",
    "        }\n",
    "        data.append(record)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Generate synthetic data\n",
    "df = generate_synthetic_expense_data()\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('synthetic_expense_data.csv', index=False)\n",
    "\n",
    "# Print sample statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Total Records: {len(df)}\")\n",
    "print(f\"Violation Rate: {(df['is_violation'].sum() / len(df)) * 100:.2f}%\")\n",
    "print(\"\\nSample Records:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy scikit-learn xgboost joblib datetime\n",
    "!pip install pandas numpy scikit-learn xgboost joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "def generate_synthetic_expense_data(num_records=1000, num_employees=50):\n",
    "    \"\"\"\n",
    "    Generate synthetic expense report data with intentional anomalies and policy violations.\n",
    "    \"\"\"\n",
    "    # Static data\n",
    "    departments = ['Sales', 'Engineering', 'Marketing', 'HR', 'Finance']\n",
    "    expense_categories = ['Travel', 'Meals', 'Office Supplies', 'Technology', 'Training']\n",
    "    vendors = ['Airlines Corp', 'Hotel Chain', 'Restaurant A', 'Office Store', 'Tech Store', \n",
    "               'Training Corp', 'Local Restaurant', 'Online Retailer']\n",
    "    payment_methods = ['Corporate Card', 'Personal Card', 'Cash']\n",
    "    \n",
    "    # Budget limits per category\n",
    "    category_limits = {\n",
    "        'Travel': 5000,\n",
    "        'Meals': 200,\n",
    "        'Office Supplies': 500,\n",
    "        'Technology': 2000,\n",
    "        'Training': 1500\n",
    "    }\n",
    "    \n",
    "    # Department-specific authorized categories\n",
    "    dept_categories = {\n",
    "        'Sales': ['Travel', 'Meals', 'Technology'],\n",
    "        'Engineering': ['Technology', 'Training', 'Office Supplies'],\n",
    "        'Marketing': ['Travel', 'Meals', 'Technology', 'Training'],\n",
    "        'HR': ['Office Supplies', 'Training'],\n",
    "        'Finance': ['Office Supplies', 'Technology']\n",
    "    }\n",
    "    \n",
    "    # Generate base data\n",
    "    data = []\n",
    "    start_date = datetime(2024, 1, 1)\n",
    "    \n",
    "    for i in range(num_records):\n",
    "        employee_id = f'EMP{random.randint(1, num_employees):03d}'\n",
    "        department = random.choice(departments)\n",
    "        submission_date = start_date + timedelta(days=random.randint(0, 365))\n",
    "        expense_date = submission_date - timedelta(days=random.randint(0, 30))\n",
    "        \n",
    "        # Intentionally create some policy violations\n",
    "        is_violation = random.random() < 0.15  # 15% chance of violation\n",
    "        \n",
    "        if is_violation:\n",
    "            # Generate a violation case\n",
    "            violation_type = random.choice(['over_budget', 'unauthorized', 'duplicate'])\n",
    "            \n",
    "            if violation_type == 'over_budget':\n",
    "                category = random.choice(list(category_limits.keys()))\n",
    "                amount = category_limits[category] * random.uniform(1.1, 2.0)  # Exceed limit\n",
    "            elif violation_type == 'unauthorized':\n",
    "                category = random.choice(expense_categories)\n",
    "                while category in dept_categories[department]:\n",
    "                    category = random.choice(expense_categories)\n",
    "                amount = random.uniform(50, 500)\n",
    "            else:  # duplicate\n",
    "                category = random.choice(expense_categories)\n",
    "                amount = random.uniform(50, category_limits[category])\n",
    "        else:\n",
    "            # Generate a normal case\n",
    "            category = random.choice(dept_categories[department])\n",
    "            amount = random.uniform(50, category_limits[category] * 0.8)\n",
    "        \n",
    "        # Add more realistic details\n",
    "        project_code = f'PRJ{random.randint(100,999)}' if random.random() > 0.7 else None\n",
    "        receipt_number = f'RCP{random.randint(10000,99999)}' if random.random() > 0.1 else None\n",
    "        \n",
    "        record = {\n",
    "            'expense_id': f'EXP{i:06d}',\n",
    "            'employee_id': employee_id,\n",
    "            'department': department,\n",
    "            'submission_date': submission_date.strftime('%Y-%m-%d'),\n",
    "            'expense_date': expense_date.strftime('%Y-%m-%d'),\n",
    "            'amount': round(amount, 2),\n",
    "            'currency': 'USD',\n",
    "            'vendor': random.choice(vendors),\n",
    "            'category': category,\n",
    "            'payment_method': random.choice(payment_methods),\n",
    "            'has_justification': random.random() > 0.1,  # 10% chance of missing justification\n",
    "            'project_code': project_code,\n",
    "            'receipt_number': receipt_number,\n",
    "            'approval_status': random.choice(['Pending', 'Approved', 'Rejected']) if amount > 1000 else 'Approved',\n",
    "            'violation_type': violation_type if is_violation else None,\n",
    "            'is_violation': is_violation\n",
    "        }\n",
    "        data.append(record)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add some basic data validation\n",
    "    df['days_to_submit'] = (pd.to_datetime(df['submission_date']) - \n",
    "                           pd.to_datetime(df['expense_date'])).dt.days\n",
    "    \n",
    "    # Generate summary statistics\n",
    "    summary_stats = {\n",
    "        'total_records': len(df),\n",
    "        'total_amount': df['amount'].sum(),\n",
    "        'average_amount': df['amount'].mean(),\n",
    "        'violation_rate': (df['is_violation'].sum() / len(df)) * 100,\n",
    "        'violations_by_type': df[df['is_violation']]['violation_type'].value_counts().to_dict(),\n",
    "        'expenses_by_department': df.groupby('department')['amount'].sum().to_dict(),\n",
    "        'average_submission_delay': df['days_to_submit'].mean()\n",
    "    }\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv('synthetic_expense_data.csv', index=False)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"Total Records: {summary_stats['total_records']}\")\n",
    "    print(f\"Total Amount: ${summary_stats['total_amount']:,.2f}\")\n",
    "    print(f\"Average Amount: ${summary_stats['average_amount']:.2f}\")\n",
    "    print(f\"Violation Rate: {summary_stats['violation_rate']:.2f}%\")\n",
    "    print(\"\\nViolations by Type:\")\n",
    "    for vtype, count in summary_stats['violations_by_type'].items():\n",
    "        print(f\"  {vtype}: {count}\")\n",
    "    print(\"\\nExpenses by Department:\")\n",
    "    for dept, amount in summary_stats['expenses_by_department'].items():\n",
    "        print(f\"  {dept}: ${amount:,.2f}\")\n",
    "    print(f\"\\nAverage Submission Delay: {summary_stats['average_submission_delay']:.1f} days\")\n",
    "    print(\"\\nSample Records:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    return df, summary_stats\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data with default parameters\n",
    "    df, stats = generate_synthetic_expense_data()\n",
    "    \n",
    "    # You can also generate with custom parameters\n",
    "    # df, stats = generate_synthetic_expense_data(num_records=2000, num_employees=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import xgboost as xgb\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "class DateFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom transformer for extracting date-based features\"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Convert date strings to datetime\n",
    "        X['submission_date'] = pd.to_datetime(X['submission_date'])\n",
    "        X['expense_date'] = pd.to_datetime(X['expense_date'])\n",
    "        \n",
    "        # Extract date features\n",
    "        X['submission_delay'] = (X['submission_date'] - X['expense_date']).dt.days\n",
    "        X['day_of_week'] = X['expense_date'].dt.dayofweek\n",
    "        X['is_weekend'] = X['day_of_week'].isin([5, 6]).astype(int)\n",
    "        X['month'] = X['expense_date'].dt.month\n",
    "        X['quarter'] = X['expense_date'].dt.quarter\n",
    "        X['is_month_end'] = X['expense_date'].dt.is_month_end.astype(int)\n",
    "        \n",
    "        # Drop original date columns\n",
    "        X = X.drop(['submission_date', 'expense_date'], axis=1)\n",
    "        \n",
    "        return X\n",
    "\n",
    "class AdvancedExpenseDetector:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.preprocessor = None\n",
    "        self.feature_importances_ = None\n",
    "        \n",
    "    def prepare_data(self, df):\n",
    "        \"\"\"Prepare data for training/prediction\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Define feature columns\n",
    "        numeric_features = [\n",
    "            'amount', 'amount_local', 'vendor_risk_score', 'receipt_quality',\n",
    "            'ocr_confidence', 'num_attendees', 'employee_risk_score', 'previous_violations'\n",
    "        ]\n",
    "        \n",
    "        categorical_features = [\n",
    "            'department', 'seniority', 'category', 'currency', 'vendor_country',\n",
    "            'payment_method', 'cost_center'\n",
    "        ]\n",
    "        \n",
    "        date_features = ['submission_date', 'expense_date']\n",
    "        \n",
    "        binary_features = [\n",
    "            'requires_approval', 'high_risk_category', 'has_receipt',\n",
    "            'manual_review_required'\n",
    "        ]\n",
    "        \n",
    "        # Create preprocessing pipeline\n",
    "        self.preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), numeric_features),\n",
    "                ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "                ('date', DateFeatureExtractor(), date_features),\n",
    "                ('bin', 'passthrough', binary_features)\n",
    "            ])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def train_models(self, df, target_column='is_violation'):\n",
    "        \"\"\"Train multiple models for expense violation detection\"\"\"\n",
    "        # Prepare data\n",
    "        df = self.prepare_data(df)\n",
    "        \n",
    "        # Remove unnecessary columns\n",
    "        columns_to_drop = ['expense_id', 'employee_id', 'vendor_id', 'vendor_name', \n",
    "                          'project_code', 'notes', 'approval_status', 'approval_date']\n",
    "        features = df.drop(columns_to_drop + [target_column], axis=1, errors='ignore')\n",
    "        target = df[target_column]\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            features, target, test_size=0.2, random_state=42, stratify=target\n",
    "        )\n",
    "        \n",
    "        # Initialize models\n",
    "        models = {\n",
    "            'random_forest': Pipeline([\n",
    "                ('preprocessor', self.preprocessor),\n",
    "                ('classifier', RandomForestClassifier(n_estimators=100, \n",
    "                                                    max_depth=10,\n",
    "                                                    class_weight='balanced',\n",
    "                                                    random_state=42))\n",
    "            ]),\n",
    "            'gradient_boosting': Pipeline([\n",
    "                ('preprocessor', self.preprocessor),\n",
    "                ('classifier', GradientBoostingClassifier(n_estimators=100,\n",
    "                                                        learning_rate=0.1,\n",
    "                                                        max_depth=5,\n",
    "                                                        random_state=42))\n",
    "            ]),\n",
    "            'xgboost': Pipeline([\n",
    "                ('preprocessor', self.preprocessor),\n",
    "                ('classifier', xgb.XGBClassifier(n_estimators=100,\n",
    "                                               learning_rate=0.1,\n",
    "                                               max_depth=5,\n",
    "                                               random_state=42))\n",
    "            ])\n",
    "        }\n",
    "        \n",
    "        # Train and evaluate each model\n",
    "        results = {}\n",
    "        for name, model in models.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            results[name] = {\n",
    "                'classification_report': classification_report(y_test, y_pred),\n",
    "                'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "                'model': model\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{name} Results:\")\n",
    "            print(results[name]['classification_report'])\n",
    "        \n",
    "        # Store the best model\n",
    "        best_model = max(results.items(), \n",
    "                        key=lambda x: float(x[1]['classification_report'].split('\\n')[-2].split()[-2]))\n",
    "        self.models['best_model'] = best_model[1]['model']\n",
    "        \n",
    "        # Store feature importances if available\n",
    "        if hasattr(self.models['best_model'].named_steps['classifier'], 'feature_importances_'):\n",
    "            feature_names = (numeric_features + \n",
    "                           self.preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features).tolist() +\n",
    "                           ['submission_delay', 'day_of_week', 'is_weekend', 'month', 'quarter', 'is_month_end'] +\n",
    "                           binary_features)\n",
    "            \n",
    "            self.feature_importances_ = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': self.models['best_model'].named_steps['classifier'].feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def predict(self, df):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        if not self.models.get('best_model'):\n",
    "            raise ValueError(\"Model has not been trained yet. Call train_models() first.\")\n",
    "        \n",
    "        # Prepare data\n",
    "        df = self.prepare_data(df)\n",
    "        \n",
    "        # Remove unnecessary columns\n",
    "        columns_to_drop = ['expense_id', 'employee_id', 'vendor_id', 'vendor_name', \n",
    "                          'project_code', 'notes', 'approval_status', 'approval_date']\n",
    "        features = df.drop(columns_to_drop + ['is_violation'], axis=1, errors='ignore')\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = self.models['best_model'].predict(features)\n",
    "        probabilities = self.models['best_model'].predict_proba(features)\n",
    "        \n",
    "        return predictions, probabilities\n",
    "    \n",
    "    def get_feature_importance(self):\n",
    "        \"\"\"Return feature importance analysis\"\"\"\n",
    "        if self.feature_importances_ is None:\n",
    "            raise ValueError(\"Feature importances not available. Train the model first.\")\n",
    "        return self.feature_importances_\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the trained model\"\"\"\n",
    "        if not self.models.get('best_model'):\n",
    "            raise ValueError(\"No trained model to save\")\n",
    "        joblib.dump(self.models['best_model'], filepath)\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load a trained model\"\"\"\n",
    "        self.models['best_model'] = joblib.load(filepath)\n",
    "        return self\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your data\n",
    "    df = pd.read_csv('synthetic_expense_data.csv')\n",
    "    \n",
    "    # Initialize and train the detector\n",
    "    detector = AdvancedExpenseDetector()\n",
    "    results = detector.train_models(df)\n",
    "    \n",
    "    # Save the best model\n",
    "    detector.save_model('expense_detector_model.joblib')\n",
    "    \n",
    "    # Get feature importance\n",
    "    try:\n",
    "        importance_df = detector.get_feature_importance()\n",
    "        print(\"\\nTop 10 Most Important Features:\")\n",
    "        print(importance_df.head(10))\n",
    "    except:\n",
    "        print(\"\\nFeature importance not available for this model\")\n",
    "    \n",
    "    # Make predictions on new data\n",
    "    predictions, probabilities = detector.predict(df.head())\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    for i, (pred, prob) in enumerate(zip(predictions, probabilities)):\n",
    "        print(f\"Record {i+1}: {'Violation' if pred else 'Normal'} (Confidence: {max(prob)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import xgboost as xgb\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "class DateFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom transformer for extracting date-based features.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Convert date strings to datetime\n",
    "        X['submission_date'] = pd.to_datetime(X['submission_date'])\n",
    "        X['expense_date'] = pd.to_datetime(X['expense_date'])\n",
    "        \n",
    "        # Extract date features\n",
    "        X['submission_delay'] = (X['submission_date'] - X['expense_date']).dt.days\n",
    "        X['day_of_week'] = X['expense_date'].dt.dayofweek\n",
    "        X['is_weekend'] = X['day_of_week'].isin([5, 6]).astype(int)\n",
    "        X['month'] = X['expense_date'].dt.month\n",
    "        X['quarter'] = X['expense_date'].dt.quarter\n",
    "        X['is_month_end'] = X['expense_date'].dt.is_month_end.astype(int)\n",
    "        \n",
    "        # Drop the original date columns\n",
    "        X = X.drop(['submission_date', 'expense_date'], axis=1)\n",
    "        return X\n",
    "\n",
    "class AdvancedExpenseDetector:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.preprocessor = None\n",
    "        self.feature_importances_ = None\n",
    "        \n",
    "        # Store feature lists as instance variables for later use\n",
    "        self.numeric_features = []\n",
    "        self.categorical_features = []\n",
    "        self.date_features = []\n",
    "        self.binary_features = []\n",
    "    \n",
    "    def prepare_data(self, df):\n",
    "        \"\"\"Prepare data for training/prediction by creating a preprocessing pipeline.\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Define all potential numeric features\n",
    "        all_numeric_features = [\n",
    "            'amount', 'amount_local', 'vendor_risk_score', 'receipt_quality',\n",
    "            'ocr_confidence', 'num_attendees', 'employee_risk_score', 'previous_violations'\n",
    "        ]\n",
    "        # Only keep those columns that actually exist in the DataFrame\n",
    "        self.numeric_features = [feat for feat in all_numeric_features if feat in df.columns]\n",
    "        \n",
    "        # Define categorical features (assumes these columns exist in your data)\n",
    "        self.categorical_features = [\n",
    "            'department', 'seniority', 'category', 'currency', 'vendor_country',\n",
    "            'payment_method', 'cost_center'\n",
    "        ]\n",
    "        \n",
    "        # Define date features (must be present for date processing)\n",
    "        self.date_features = ['submission_date', 'expense_date']\n",
    "        \n",
    "        # Define binary features (again, check these exist in your data)\n",
    "        self.binary_features = [\n",
    "            'requires_approval', 'high_risk_category', 'has_receipt',\n",
    "            'manual_review_required'\n",
    "        ]\n",
    "        \n",
    "        # Create the preprocessing pipeline\n",
    "        self.preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), self.numeric_features),\n",
    "                ('cat', OneHotEncoder(handle_unknown='ignore'), self.categorical_features),\n",
    "                ('date', DateFeatureExtractor(), self.date_features),\n",
    "                ('bin', 'passthrough', self.binary_features)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def train_models(self, df, target_column='is_violation'):\n",
    "        \"\"\"Train multiple models for expense violation detection.\"\"\"\n",
    "        # Prepare the data and build the preprocessor\n",
    "        df = self.prepare_data(df)\n",
    "        \n",
    "        # Remove unnecessary columns\n",
    "        columns_to_drop = ['expense_id', 'employee_id', 'vendor_id', 'vendor_name', \n",
    "                           'project_code', 'notes', 'approval_status', 'approval_date']\n",
    "        features = df.drop(columns=columns_to_drop + [target_column], axis=1, errors='ignore')\n",
    "        target = df[target_column]\n",
    "        \n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            features, target, test_size=0.2, random_state=42, stratify=target\n",
    "        )\n",
    "        \n",
    "        # Initialize the models within pipelines\n",
    "        models = {\n",
    "            'random_forest': Pipeline([\n",
    "                ('preprocessor', self.preprocessor),\n",
    "                ('classifier', RandomForestClassifier(n_estimators=100, \n",
    "                                                        max_depth=10,\n",
    "                                                        class_weight='balanced',\n",
    "                                                        random_state=42))\n",
    "            ]),\n",
    "            'gradient_boosting': Pipeline([\n",
    "                ('preprocessor', self.preprocessor),\n",
    "                ('classifier', GradientBoostingClassifier(n_estimators=100,\n",
    "                                                          learning_rate=0.1,\n",
    "                                                          max_depth=5,\n",
    "                                                          random_state=42))\n",
    "            ]),\n",
    "            'xgboost': Pipeline([\n",
    "                ('preprocessor', self.preprocessor),\n",
    "                ('classifier', xgb.XGBClassifier(n_estimators=100,\n",
    "                                                 learning_rate=0.1,\n",
    "                                                 max_depth=5,\n",
    "                                                 random_state=42))\n",
    "            ])\n",
    "        }\n",
    "        \n",
    "        # Train each model and evaluate performance\n",
    "        results = {}\n",
    "        for name, model in models.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Make predictions on the test set\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics and store results\n",
    "            results[name] = {\n",
    "                'classification_report': classification_report(y_test, y_pred),\n",
    "                'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "                'model': model\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{name} Results:\")\n",
    "            print(results[name]['classification_report'])\n",
    "        \n",
    "        # Store the best model (here, we simply choose the one with highest overall F1 score)\n",
    "        # Note: Adjust the selection criteria as needed.\n",
    "        best_model = max(results.items(), key=lambda x: float(x[1]['classification_report'].split()[-2]))\n",
    "        self.models['best_model'] = best_model[1]['model']\n",
    "        \n",
    "        # Compute feature importances if the classifier supports it\n",
    "        classifier = self.models['best_model'].named_steps['classifier']\n",
    "        if hasattr(classifier, 'feature_importances_'):\n",
    "            # Get one-hot encoder feature names\n",
    "            cat_feature_names = self.preprocessor.named_transformers_['cat'] \\\n",
    "                                .get_feature_names_out(self.categorical_features).tolist()\n",
    "            # Combine all feature names in the order they are passed to the classifier\n",
    "            feature_names = (self.numeric_features +\n",
    "                             cat_feature_names +\n",
    "                             ['submission_delay', 'day_of_week', 'is_weekend', 'month', 'quarter', 'is_month_end'] +\n",
    "                             self.binary_features)\n",
    "            \n",
    "            self.feature_importances_ = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': classifier.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def predict(self, df):\n",
    "        \"\"\"Make predictions on new data.\"\"\"\n",
    "        if 'best_model' not in self.models:\n",
    "            raise ValueError(\"Model has not been trained yet. Call train_models() first.\")\n",
    "        \n",
    "        # Prepare data using the same preprocessor\n",
    "        df = self.prepare_data(df)\n",
    "        columns_to_drop = ['expense_id', 'employee_id', 'vendor_id', 'vendor_name', \n",
    "                           'project_code', 'notes', 'approval_status', 'approval_date']\n",
    "        features = df.drop(columns=columns_to_drop + ['is_violation'], axis=1, errors='ignore')\n",
    "        \n",
    "        predictions = self.models['best_model'].predict(features)\n",
    "        probabilities = self.models['best_model'].predict_proba(features)\n",
    "        return predictions, probabilities\n",
    "    \n",
    "    def get_feature_importance(self):\n",
    "        \"\"\"Return feature importance analysis.\"\"\"\n",
    "        if self.feature_importances_ is None:\n",
    "            raise ValueError(\"Feature importances not available. Train the model first.\")\n",
    "        return self.feature_importances_\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the trained model to disk.\"\"\"\n",
    "        if 'best_model' not in self.models:\n",
    "            raise ValueError(\"No trained model to save.\")\n",
    "        joblib.dump(self.models['best_model'], filepath)\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load a trained model from disk.\"\"\"\n",
    "        self.models['best_model'] = joblib.load(filepath)\n",
    "        return self\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your synthetic data CSV file\n",
    "    df = pd.read_csv('synthetic_expense_data.csv')\n",
    "    \n",
    "    # Initialize and train the detector\n",
    "    detector = AdvancedExpenseDetector()\n",
    "    results = detector.train_models(df)\n",
    "    \n",
    "    # Save the best model\n",
    "    detector.save_model('expense_detector_model.joblib')\n",
    "    \n",
    "    # Get and print feature importances if available\n",
    "    try:\n",
    "        importance_df = detector.get_feature_importance()\n",
    "        print(\"\\nTop 10 Most Important Features:\")\n",
    "        print(importance_df.head(10))\n",
    "    except Exception as e:\n",
    "        print(\"\\nFeature importance not available:\", e)\n",
    "    \n",
    "    # Make predictions on a sample of the data\n",
    "    predictions, probabilities = detector.predict(df.head())\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    for i, (pred, prob) in enumerate(zip(predictions, probabilities)):\n",
    "        label = 'Violation' if pred else 'Normal'\n",
    "        confidence = max(prob) * 100\n",
    "        print(f\"Record {i+1}: {label} (Confidence: {confidence:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data sample:\n",
      "   expense_id  employee_id  amount  amount_local  vendor_risk_score  \\\n",
      "0           1         1102  698.56        648.03               0.60   \n",
      "1           2         1435  147.94        137.07               0.27   \n",
      "2           3         1860  608.37        665.50               0.13   \n",
      "3           4         1270  544.44        567.81               0.08   \n",
      "4           5         1106  211.03        191.66               0.94   \n",
      "\n",
      "   receipt_quality  ocr_confidence  num_attendees  employee_risk_score  \\\n",
      "0             0.55            0.64              4                 0.04   \n",
      "1             0.55            0.70              1                 0.67   \n",
      "2             0.20            0.91              3                 0.95   \n",
      "3             0.68            0.62              2                 0.12   \n",
      "4             0.09            0.34              2                 0.90   \n",
      "\n",
      "   previous_violations  ... high_risk_category has_receipt  \\\n",
      "0                    1  ...                  0           0   \n",
      "1                    1  ...                  1           0   \n",
      "2                    0  ...                  1           0   \n",
      "3                    2  ...                  1           1   \n",
      "4                    4  ...                  1           1   \n",
      "\n",
      "  manual_review_required vendor_id vendor_name project_code notes  \\\n",
      "0                      0      2480     VendorA         P100         \n",
      "1                      1      2351     VendorA         P100         \n",
      "2                      0      2559     VendorA         P200         \n",
      "3                      1      2325     VendorA         P300         \n",
      "4                      1      2744     VendorB         P100         \n",
      "\n",
      "  approval_status  approval_date  is_violation  \n",
      "0        Approved     2025-02-08             0  \n",
      "1        Approved     2025-02-08             1  \n",
      "2        Approved     2025-02-08             0  \n",
      "3        Rejected     2025-02-08             0  \n",
      "4        Rejected     2025-02-08             0  \n",
      "\n",
      "[5 rows x 29 columns] \n",
      "\n",
      "\n",
      "Training random_forest...\n",
      "\n",
      "random_forest Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89        32\n",
      "           1       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.80        40\n",
      "   macro avg       0.40      0.50      0.44        40\n",
      "weighted avg       0.64      0.80      0.71        40\n",
      "\n",
      "\n",
      "Training gradient_boosting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Bhavi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Bhavi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gradient_boosting Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.97      0.87        32\n",
      "           1       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.78        40\n",
      "   macro avg       0.40      0.48      0.44        40\n",
      "weighted avg       0.64      0.78      0.70        40\n",
      "\n",
      "\n",
      "Training xgboost...\n",
      "\n",
      "xgboost Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.91      0.84        32\n",
      "           1       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.72        40\n",
      "   macro avg       0.39      0.45      0.42        40\n",
      "weighted avg       0.63      0.72      0.67        40\n",
      "\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                feature  importance\n",
      "2     vendor_risk_score    0.090091\n",
      "1          amount_local    0.087846\n",
      "6   employee_risk_score    0.082312\n",
      "0                amount    0.082061\n",
      "3       receipt_quality    0.078574\n",
      "4        ocr_confidence    0.069882\n",
      "5         num_attendees    0.055769\n",
      "27          day_of_week    0.051241\n",
      "26     submission_delay    0.048677\n",
      "7   previous_violations    0.037637\n",
      "\n",
      "Sample Predictions:\n",
      "Record 1: Normal (Confidence: 95.00%)\n",
      "Record 2: Violation (Confidence: 72.00%)\n",
      "Record 3: Normal (Confidence: 78.00%)\n",
      "Record 4: Normal (Confidence: 88.00%)\n",
      "Record 5: Normal (Confidence: 85.26%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\xgboost\\core.py:158: UserWarning: [18:12:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# ----------------------------\n",
    "# Custom Transformer: DateFeatureExtractor\n",
    "# ----------------------------\n",
    "class DateFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract date-based features from submission_date and expense_date.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Convert date strings to datetime\n",
    "        X['submission_date'] = pd.to_datetime(X['submission_date'])\n",
    "        X['expense_date'] = pd.to_datetime(X['expense_date'])\n",
    "        \n",
    "        # Extract date-based features\n",
    "        X['submission_delay'] = (X['submission_date'] - X['expense_date']).dt.days\n",
    "        X['day_of_week'] = X['expense_date'].dt.dayofweek\n",
    "        X['is_weekend'] = X['day_of_week'].isin([5, 6]).astype(int)\n",
    "        X['month'] = X['expense_date'].dt.month\n",
    "        X['quarter'] = X['expense_date'].dt.quarter\n",
    "        X['is_month_end'] = X['expense_date'].dt.is_month_end.astype(int)\n",
    "        \n",
    "        # Drop the original date columns\n",
    "        X = X.drop(['submission_date', 'expense_date'], axis=1)\n",
    "        return X\n",
    "\n",
    "# ----------------------------\n",
    "# Detector Class: AdvancedExpenseDetector\n",
    "# ----------------------------\n",
    "class AdvancedExpenseDetector:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.preprocessor = None\n",
    "        self.feature_importances_ = None\n",
    "\n",
    "        # Initialize feature lists (will be filtered based on input DataFrame)\n",
    "        self.numeric_features = []\n",
    "        self.categorical_features = []\n",
    "        self.date_features = []\n",
    "        self.binary_features = []\n",
    "\n",
    "    def prepare_data(self, df):\n",
    "        \"\"\"Prepare data by setting up the preprocessing pipeline, filtering out missing columns.\"\"\"\n",
    "        df = df.copy()\n",
    "\n",
    "        # Define potential feature lists\n",
    "        potential_numeric_features = [\n",
    "            'amount', 'amount_local', 'vendor_risk_score', 'receipt_quality',\n",
    "            'ocr_confidence', 'num_attendees', 'employee_risk_score', 'previous_violations'\n",
    "        ]\n",
    "        potential_categorical_features = [\n",
    "            'department', 'seniority', 'category', 'currency', 'vendor_country',\n",
    "            'payment_method', 'cost_center'\n",
    "        ]\n",
    "        potential_date_features = ['submission_date', 'expense_date']\n",
    "        potential_binary_features = [\n",
    "            'requires_approval', 'high_risk_category', 'has_receipt', 'manual_review_required'\n",
    "        ]\n",
    "\n",
    "        # Filter features based on the columns present in the DataFrame\n",
    "        self.numeric_features = [col for col in potential_numeric_features if col in df.columns]\n",
    "        self.categorical_features = [col for col in potential_categorical_features if col in df.columns]\n",
    "        self.date_features = [col for col in potential_date_features if col in df.columns]\n",
    "        self.binary_features = [col for col in potential_binary_features if col in df.columns]\n",
    "\n",
    "        # Create the preprocessing pipeline\n",
    "        self.preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), self.numeric_features),\n",
    "                ('cat', OneHotEncoder(handle_unknown='ignore'), self.categorical_features),\n",
    "                ('date', DateFeatureExtractor(), self.date_features),\n",
    "                ('bin', 'passthrough', self.binary_features)\n",
    "            ]\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def train_models(self, df, target_column='is_violation'):\n",
    "        \"\"\"Train multiple models to detect expense violations.\"\"\"\n",
    "        # Prepare the data and set up the preprocessor\n",
    "        df = self.prepare_data(df)\n",
    "\n",
    "        # Drop columns that are not used as features\n",
    "        columns_to_drop = ['expense_id', 'employee_id', 'vendor_id', 'vendor_name', \n",
    "                           'project_code', 'notes', 'approval_status', 'approval_date']\n",
    "        features = df.drop(columns=columns_to_drop + [target_column], axis=1, errors='ignore')\n",
    "        target = df[target_column]\n",
    "\n",
    "        # Split the dataset into training and testing subsets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            features, target, test_size=0.2, random_state=42, stratify=target\n",
    "        )\n",
    "\n",
    "        # Define models inside pipelines\n",
    "        models = {\n",
    "            'random_forest': Pipeline([\n",
    "                ('preprocessor', self.preprocessor),\n",
    "                ('classifier', RandomForestClassifier(n_estimators=100, \n",
    "                                                        max_depth=10,\n",
    "                                                        class_weight='balanced',\n",
    "                                                        random_state=42))\n",
    "            ]),\n",
    "            'gradient_boosting': Pipeline([\n",
    "                ('preprocessor', self.preprocessor),\n",
    "                ('classifier', GradientBoostingClassifier(n_estimators=100,\n",
    "                                                          learning_rate=0.1,\n",
    "                                                          max_depth=5,\n",
    "                                                          random_state=42))\n",
    "            ]),\n",
    "            'xgboost': Pipeline([\n",
    "                ('preprocessor', self.preprocessor),\n",
    "                ('classifier', xgb.XGBClassifier(n_estimators=100,\n",
    "                                                 learning_rate=0.1,\n",
    "                                                 max_depth=5,\n",
    "                                                 use_label_encoder=False,\n",
    "                                                 eval_metric='logloss',\n",
    "                                                 random_state=42))\n",
    "            ])\n",
    "        }\n",
    "\n",
    "        results = {}\n",
    "        # Train and evaluate each model\n",
    "        for name, model in models.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            results[name] = {\n",
    "                'classification_report': classification_report(y_test, y_pred),\n",
    "                'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "                'model': model\n",
    "            }\n",
    "            print(f\"\\n{name} Results:\")\n",
    "            print(results[name]['classification_report'])\n",
    "\n",
    "        # Choose the best model (here, we simply select the first one)\n",
    "        self.models['best_model'] = models['random_forest']\n",
    "\n",
    "        # If the classifier supports feature importances, compute them.\n",
    "        classifier = self.models['best_model'].named_steps['classifier']\n",
    "        if hasattr(classifier, 'feature_importances_'):\n",
    "            cat_feature_names = []\n",
    "            if self.categorical_features:\n",
    "                cat_feature_names = self.preprocessor.named_transformers_['cat'].get_feature_names_out(self.categorical_features).tolist()\n",
    "            # Combine feature names (order must match transformation order)\n",
    "            feature_names = (\n",
    "                self.numeric_features +\n",
    "                cat_feature_names +\n",
    "                ['submission_delay', 'day_of_week', 'is_weekend', 'month', 'quarter', 'is_month_end'] +\n",
    "                self.binary_features\n",
    "            )\n",
    "            self.feature_importances_ = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': classifier.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def predict(self, df):\n",
    "        \"\"\"Make predictions on new data using the best model.\"\"\"\n",
    "        if 'best_model' not in self.models:\n",
    "            raise ValueError(\"Model has not been trained yet. Call train_models() first.\")\n",
    "        df = self.prepare_data(df)\n",
    "        columns_to_drop = ['expense_id', 'employee_id', 'vendor_id', 'vendor_name', \n",
    "                           'project_code', 'notes', 'approval_status', 'approval_date']\n",
    "        features = df.drop(columns=columns_to_drop + ['is_violation'], axis=1, errors='ignore')\n",
    "        predictions = self.models['best_model'].predict(features)\n",
    "        probabilities = self.models['best_model'].predict_proba(features)\n",
    "        return predictions, probabilities\n",
    "\n",
    "    def get_feature_importance(self):\n",
    "        \"\"\"Return the computed feature importances (if available).\"\"\"\n",
    "        if self.feature_importances_ is None:\n",
    "            raise ValueError(\"Feature importances not available. Train the model first.\")\n",
    "        return self.feature_importances_\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the trained best model to disk.\"\"\"\n",
    "        if 'best_model' not in self.models:\n",
    "            raise ValueError(\"No trained model to save.\")\n",
    "        joblib.dump(self.models['best_model'], filepath)\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load a trained model from disk.\"\"\"\n",
    "        self.models['best_model'] = joblib.load(filepath)\n",
    "        return self\n",
    "\n",
    "# ----------------------------\n",
    "# Synthetic Data Generation for Testing\n",
    "# ----------------------------\n",
    "def generate_synthetic_data(n_samples=100):\n",
    "    np.random.seed(42)\n",
    "    data = {}\n",
    "    \n",
    "    # ID and employee columns\n",
    "    data['expense_id'] = np.arange(1, n_samples + 1)\n",
    "    data['employee_id'] = np.random.randint(1000, 2000, n_samples)\n",
    "    \n",
    "    # Numeric features\n",
    "    data['amount'] = np.random.uniform(10, 1000, n_samples).round(2)\n",
    "    # For demonstration, we include amount_local here; you can remove it to test missing column handling.\n",
    "    data['amount_local'] = (data['amount'] * np.random.uniform(0.9, 1.1, n_samples)).round(2)\n",
    "    data['vendor_risk_score'] = np.random.uniform(0, 1, n_samples).round(2)\n",
    "    data['receipt_quality'] = np.random.uniform(0, 1, n_samples).round(2)\n",
    "    data['ocr_confidence'] = np.random.uniform(0, 1, n_samples).round(2)\n",
    "    data['num_attendees'] = np.random.randint(1, 10, n_samples)\n",
    "    data['employee_risk_score'] = np.random.uniform(0, 1, n_samples).round(2)\n",
    "    data['previous_violations'] = np.random.randint(0, 5, n_samples)\n",
    "    \n",
    "    # Categorical features (intentionally leaving out \"seniority\" to test our fix)\n",
    "    data['department'] = np.random.choice(['Sales', 'Engineering', 'HR', 'Marketing'], n_samples)\n",
    "    # data['seniority'] is intentionally omitted to simulate a missing column.\n",
    "    data['category'] = np.random.choice(['Travel', 'Meals', 'Supplies'], n_samples)\n",
    "    data['currency'] = np.random.choice(['USD', 'EUR'], n_samples)\n",
    "    data['vendor_country'] = np.random.choice(['US', 'FR', 'DE'], n_samples)\n",
    "    data['payment_method'] = np.random.choice(['Credit Card', 'Cash', 'Wire Transfer'], n_samples)\n",
    "    data['cost_center'] = np.random.choice(['A1', 'B2', 'C3'], n_samples)\n",
    "    \n",
    "    # Date features\n",
    "    base_date = datetime.today()\n",
    "    data['expense_date'] = [(base_date - timedelta(days=np.random.randint(1, 30))).strftime('%Y-%m-%d') for _ in range(n_samples)]\n",
    "    data['submission_date'] = [(datetime.strptime(exp_date, '%Y-%m-%d') + timedelta(days=np.random.randint(0, 10))).strftime('%Y-%m-%d') for exp_date in data['expense_date']]\n",
    "    \n",
    "    # Binary features\n",
    "    data['requires_approval'] = np.random.choice([0, 1], n_samples)\n",
    "    data['high_risk_category'] = np.random.choice([0, 1], n_samples)\n",
    "    data['has_receipt'] = np.random.choice([0, 1], n_samples)\n",
    "    data['manual_review_required'] = np.random.choice([0, 1], n_samples)\n",
    "    \n",
    "    # Other non-feature columns\n",
    "    data['vendor_id'] = np.random.randint(2000, 3000, n_samples)\n",
    "    data['vendor_name'] = np.random.choice(['VendorA', 'VendorB', 'VendorC'], n_samples)\n",
    "    data['project_code'] = np.random.choice(['P100', 'P200', 'P300'], n_samples)\n",
    "    data['notes'] = [''] * n_samples\n",
    "    data['approval_status'] = np.random.choice(['Approved', 'Rejected'], n_samples)\n",
    "    data['approval_date'] = [base_date.strftime('%Y-%m-%d')] * n_samples\n",
    "    \n",
    "    # Target column\n",
    "    data['is_violation'] = np.random.choice([0, 1], n_samples, p=[0.8, 0.2])\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# Main Execution\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data\n",
    "    df = generate_synthetic_data(n_samples=200)\n",
    "    print(\"Synthetic data sample:\")\n",
    "    print(df.head(), \"\\n\")\n",
    "    \n",
    "    # Initialize and train the detector\n",
    "    detector = AdvancedExpenseDetector()\n",
    "    results = detector.train_models(df)\n",
    "    \n",
    "    # Optionally, save the best model\n",
    "    detector.save_model('expense_detector_model.joblib')\n",
    "    \n",
    "    # Print feature importances if available\n",
    "    try:\n",
    "        importance_df = detector.get_feature_importance()\n",
    "        print(\"\\nTop 10 Most Important Features:\")\n",
    "        print(importance_df.head(10))\n",
    "    except Exception as e:\n",
    "        print(\"\\nFeature importance not available:\", e)\n",
    "    \n",
    "    # Make predictions on a sample of the data\n",
    "    predictions, probabilities = detector.predict(df.head())\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    for i, (pred, prob) in enumerate(zip(predictions, probabilities)):\n",
    "        label = 'Violation' if pred else 'Normal'\n",
    "        confidence = max(prob) * 100\n",
    "        print(f\"Record {i+1}: {label} (Confidence: {confidence:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required Inputs by Type:\n",
      "  Numeric: ['amount', 'amount_local', 'vendor_risk_score', 'receipt_quality', 'ocr_confidence', 'num_attendees', 'employee_risk_score', 'previous_violations']\n",
      "  Categorical: ['department', 'seniority', 'category', 'currency', 'vendor_country', 'payment_method', 'cost_center']\n",
      "  Date: ['submission_date', 'expense_date']\n",
      "  Binary: ['requires_approval', 'high_risk_category', 'has_receipt', 'manual_review_required']\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Classification Report on Test Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89        32\n",
      "           1       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.80        40\n",
      "   macro avg       0.40      0.50      0.44        40\n",
      "weighted avg       0.64      0.80      0.71        40\n",
      "\n",
      "Confusion Matrix:\n",
      "[[32  0]\n",
      " [ 8  0]]\n",
      "Model saved to expense_detector_model.joblib\n",
      "\n",
      "Policy Compliance Report\n",
      "* Expense ID: 201\n",
      "* Employee ID: 1500 | Department: Engineering\n",
      "* Expense Category & Amount: Travel - 7500000000000.0\n",
      "* No Violations Detected.\n",
      "* Model Confidence: 76.03%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Bhavi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Bhavi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import joblib\n",
    "\n",
    "# ----------------------------\n",
    "# Custom Transformer for Date Features\n",
    "# ----------------------------\n",
    "class DateFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract date-based features from submission_date and expense_date.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['submission_date'] = pd.to_datetime(X['submission_date'])\n",
    "        X['expense_date'] = pd.to_datetime(X['expense_date'])\n",
    "        \n",
    "        # Create additional features\n",
    "        X['submission_delay'] = (X['submission_date'] - X['expense_date']).dt.days\n",
    "        X['day_of_week'] = X['expense_date'].dt.dayofweek\n",
    "        X['is_weekend'] = X['day_of_week'].isin([5, 6]).astype(int)\n",
    "        X['month'] = X['expense_date'].dt.month\n",
    "        X['quarter'] = X['expense_date'].dt.quarter\n",
    "        X['is_month_end'] = X['expense_date'].dt.is_month_end.astype(int)\n",
    "        \n",
    "        # Drop original date columns\n",
    "        X = X.drop(['submission_date', 'expense_date'], axis=1)\n",
    "        return X\n",
    "\n",
    "# ----------------------------\n",
    "# Advanced Expense Detector Class\n",
    "# ----------------------------\n",
    "class AdvancedExpenseDetector:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.preprocessor = None\n",
    "        self.feature_importances_ = None\n",
    "\n",
    "        # Define the potential input features (if available in the data)\n",
    "        self.potential_numeric = [\n",
    "            'amount', 'amount_local', 'vendor_risk_score', 'receipt_quality',\n",
    "            'ocr_confidence', 'num_attendees', 'employee_risk_score', 'previous_violations'\n",
    "        ]\n",
    "        self.potential_categorical = [\n",
    "            'department', 'seniority', 'category', 'currency', 'vendor_country',\n",
    "            'payment_method', 'cost_center'\n",
    "        ]\n",
    "        self.potential_date = ['submission_date', 'expense_date']\n",
    "        self.potential_binary = [\n",
    "            'requires_approval', 'high_risk_category', 'has_receipt', 'manual_review_required'\n",
    "        ]\n",
    "        \n",
    "    def list_required_inputs(self):\n",
    "        \"\"\"\n",
    "        Return a dictionary of required inputs by type.\n",
    "        (Columns will be used only if present in the input DataFrame.)\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'numeric': self.potential_numeric,\n",
    "            'categorical': self.potential_categorical,\n",
    "            'date': self.potential_date,\n",
    "            'binary': self.potential_binary\n",
    "        }\n",
    "    \n",
    "    def prepare_data(self, df):\n",
    "        \"\"\"Prepare data by filtering available columns and setting up the preprocessor.\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Filter the columns that exist in the DataFrame.\n",
    "        self.numeric_features = [col for col in self.potential_numeric if col in df.columns]\n",
    "        self.categorical_features = [col for col in self.potential_categorical if col in df.columns]\n",
    "        self.date_features = [col for col in self.potential_date if col in df.columns]\n",
    "        self.binary_features = [col for col in self.potential_binary if col in df.columns]\n",
    "        \n",
    "        # Create the preprocessor pipeline\n",
    "        self.preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), self.numeric_features),\n",
    "                ('cat', OneHotEncoder(handle_unknown='ignore'), self.categorical_features),\n",
    "                ('date', DateFeatureExtractor(), self.date_features),\n",
    "                ('bin', 'passthrough', self.binary_features)\n",
    "            ]\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def train_models(self, df, target_column='is_violation'):\n",
    "        \"\"\"Train a Random Forest model for expense violation detection.\"\"\"\n",
    "        # Prepare data (and build the preprocessor)\n",
    "        df = self.prepare_data(df)\n",
    "        \n",
    "        # Drop columns that are not features\n",
    "        columns_to_drop = ['expense_id', 'employee_id', 'vendor_id', 'vendor_name', \n",
    "                           'project_code', 'notes', 'approval_status', 'approval_date']\n",
    "        features = df.drop(columns=columns_to_drop + [target_column], axis=1, errors='ignore')\n",
    "        target = df[target_column]\n",
    "        \n",
    "        # Split into training and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            features, target, test_size=0.2, random_state=42, stratify=target\n",
    "        )\n",
    "        \n",
    "        # Define a Random Forest pipeline\n",
    "        model = Pipeline([\n",
    "            ('preprocessor', self.preprocessor),\n",
    "            ('classifier', RandomForestClassifier(n_estimators=100, max_depth=10,\n",
    "                                                    class_weight='balanced', random_state=42))\n",
    "        ])\n",
    "        \n",
    "        print(\"Training Random Forest...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(\"\\nClassification Report on Test Data:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        \n",
    "        # Save this model as the best model\n",
    "        self.models['best_model'] = model\n",
    "        \n",
    "        # Save feature importances if available\n",
    "        classifier = self.models['best_model'].named_steps['classifier']\n",
    "        if hasattr(classifier, 'feature_importances_'):\n",
    "            cat_feature_names = []\n",
    "            if self.categorical_features:\n",
    "                cat_feature_names = self.preprocessor.named_transformers_['cat'] \\\n",
    "                                        .get_feature_names_out(self.categorical_features).tolist()\n",
    "            # Combine feature names in the order they appear to the classifier\n",
    "            feature_names = (self.numeric_features +\n",
    "                             cat_feature_names +\n",
    "                             ['submission_delay', 'day_of_week', 'is_weekend', 'month', 'quarter', 'is_month_end'] +\n",
    "                             self.binary_features)\n",
    "            self.feature_importances_ = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': classifier.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the trained best model to disk.\"\"\"\n",
    "        if 'best_model' not in self.models:\n",
    "            raise ValueError(\"No trained model to save.\")\n",
    "        joblib.dump(self.models['best_model'], filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load a trained model from disk.\"\"\"\n",
    "        self.models['best_model'] = joblib.load(filepath)\n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, df):\n",
    "        \"\"\"Predict on a DataFrame (can be multiple rows).\"\"\"\n",
    "        if 'best_model' not in self.models:\n",
    "            raise ValueError(\"Model has not been trained yet. Call train_models() first.\")\n",
    "        df = self.prepare_data(df)\n",
    "        columns_to_drop = ['expense_id', 'employee_id', 'vendor_id', 'vendor_name', \n",
    "                           'project_code', 'notes', 'approval_status', 'approval_date']\n",
    "        features = df.drop(columns=columns_to_drop + ['is_violation'], axis=1, errors='ignore')\n",
    "        predictions = self.models['best_model'].predict(features)\n",
    "        probabilities = self.models['best_model'].predict_proba(features)\n",
    "        return predictions, probabilities\n",
    "    \n",
    "    def predict_single(self, new_row):\n",
    "        \"\"\"\n",
    "        Accept a single expense report (as a dict), predict if it's a violation,\n",
    "        and return a compliance report.\n",
    "        \"\"\"\n",
    "        # Convert the dict into a DataFrame (with one row)\n",
    "        df_single = pd.DataFrame([new_row])\n",
    "        pred, prob = self.predict(df_single)\n",
    "        prediction = pred[0]\n",
    "        probability = max(prob[0])  # highest confidence score\n",
    "        \n",
    "        report = self.get_compliance_report(new_row, prediction, probability)\n",
    "        return report\n",
    "    \n",
    "    def get_compliance_report(self, input_data, prediction, probability):\n",
    "        \"\"\"\n",
    "        Generate a compliance report based on the input data and model prediction.\n",
    "        (Here we simply flag based on the predicted class.)\n",
    "        \"\"\"\n",
    "        expense_id = input_data.get('expense_id', 'N/A')\n",
    "        employee_id = input_data.get('employee_id', 'N/A')\n",
    "        department = input_data.get('department', 'N/A')\n",
    "        category = input_data.get('category', 'N/A')\n",
    "        amount = input_data.get('amount', 'N/A')\n",
    "        \n",
    "        # Create a basic report.\n",
    "        report = f\"\\nPolicy Compliance Report\\n\"\n",
    "        report += f\"* Expense ID: {expense_id}\\n\"\n",
    "        report += f\"* Employee ID: {employee_id} | Department: {department}\\n\"\n",
    "        report += f\"* Expense Category & Amount: {category} - {amount}\\n\"\n",
    "        \n",
    "        if prediction == 1:\n",
    "            report += \"* Detected Violations:\\n\"\n",
    "            report += \"   - Violation Detected (Model flagged this expense)\\n\"\n",
    "            report += \"* Suggested Actions:\\n\"\n",
    "            report += \"   - Please review the expense policy and provide necessary justification.\\n\"\n",
    "            report += \"   - Manager review required.\\n\"\n",
    "        else:\n",
    "            report += \"* No Violations Detected.\\n\"\n",
    "        \n",
    "        report += f\"* Model Confidence: {probability*100:.2f}%\\n\"\n",
    "        return report\n",
    "\n",
    "# ----------------------------\n",
    "# Synthetic Data Generation (for training)\n",
    "# ----------------------------\n",
    "def generate_synthetic_data(n_samples=200):\n",
    "    np.random.seed(42)\n",
    "    data = {}\n",
    "    \n",
    "    # IDs and basic information\n",
    "    data['expense_id'] = np.arange(1, n_samples + 1)\n",
    "    data['employee_id'] = np.random.randint(1000, 2000, n_samples)\n",
    "    \n",
    "    # Numeric features\n",
    "    data['amount'] = np.random.uniform(10, 1000, n_samples).round(2)\n",
    "    data['amount_local'] = (data['amount'] * np.random.uniform(0.9, 1.1, n_samples)).round(2)\n",
    "    data['vendor_risk_score'] = np.random.uniform(0, 1, n_samples).round(2)\n",
    "    data['receipt_quality'] = np.random.uniform(0, 1, n_samples).round(2)\n",
    "    data['ocr_confidence'] = np.random.uniform(0, 1, n_samples).round(2)\n",
    "    data['num_attendees'] = np.random.randint(1, 10, n_samples)\n",
    "    data['employee_risk_score'] = np.random.uniform(0, 1, n_samples).round(2)\n",
    "    data['previous_violations'] = np.random.randint(0, 5, n_samples)\n",
    "    \n",
    "    # Categorical features (note: \"seniority\" is intentionally omitted to test handling)\n",
    "    data['department'] = np.random.choice(['Sales', 'Engineering', 'HR', 'Marketing'], n_samples)\n",
    "    data['category'] = np.random.choice(['Travel', 'Meals', 'Supplies'], n_samples)\n",
    "    data['currency'] = np.random.choice(['USD', 'EUR'], n_samples)\n",
    "    data['vendor_country'] = np.random.choice(['US', 'FR', 'DE'], n_samples)\n",
    "    data['payment_method'] = np.random.choice(['Credit Card', 'Cash', 'Wire Transfer'], n_samples)\n",
    "    data['cost_center'] = np.random.choice(['A1', 'B2', 'C3'], n_samples)\n",
    "    \n",
    "    # Date features\n",
    "    base_date = datetime.today()\n",
    "    data['expense_date'] = [(base_date - timedelta(days=np.random.randint(1, 30))).strftime('%Y-%m-%d')\n",
    "                            for _ in range(n_samples)]\n",
    "    data['submission_date'] = [(datetime.strptime(exp_date, '%Y-%m-%d') +\n",
    "                                timedelta(days=np.random.randint(0, 10))).strftime('%Y-%m-%d')\n",
    "                               for exp_date in data['expense_date']]\n",
    "    \n",
    "    # Binary features\n",
    "    data['requires_approval'] = np.random.choice([0, 1], n_samples)\n",
    "    data['high_risk_category'] = np.random.choice([0, 1], n_samples)\n",
    "    data['has_receipt'] = np.random.choice([0, 1], n_samples)\n",
    "    data['manual_review_required'] = np.random.choice([0, 1], n_samples)\n",
    "    \n",
    "    # Other non-feature columns\n",
    "    data['vendor_id'] = np.random.randint(2000, 3000, n_samples)\n",
    "    data['vendor_name'] = np.random.choice(['VendorA', 'VendorB', 'VendorC'], n_samples)\n",
    "    data['project_code'] = np.random.choice(['P100', 'P200', 'P300'], n_samples)\n",
    "    data['notes'] = [''] * n_samples\n",
    "    data['approval_status'] = np.random.choice(['Approved', 'Rejected'], n_samples)\n",
    "    data['approval_date'] = [base_date.strftime('%Y-%m-%d')] * n_samples\n",
    "    \n",
    "    # Target column: 0 = Normal, 1 = Violation (imbalance: ~80% Normal)\n",
    "    data['is_violation'] = np.random.choice([0, 1], n_samples, p=[0.8, 0.2])\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# ----------------------------\n",
    "# Main Execution\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. List Required Inputs\n",
    "    detector = AdvancedExpenseDetector()\n",
    "    req_inputs = detector.list_required_inputs()\n",
    "    print(\"Required Inputs by Type:\")\n",
    "    for key, val in req_inputs.items():\n",
    "        print(f\"  {key.capitalize()}: {val}\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # 2. Train the model on synthetic data\n",
    "    df_train = generate_synthetic_data(n_samples=200)\n",
    "    detector.train_models(df_train)\n",
    "    \n",
    "    # 3. Save the trained model to disk\n",
    "    detector.save_model('expense_detector_model.joblib')\n",
    "    \n",
    "    # 4. Example: Predict on a single new expense row\n",
    "    new_expense = {\n",
    "        'expense_id': 201,\n",
    "        'employee_id': 1500,\n",
    "        'amount': 7500000000000.00,\n",
    "        # 'amount_local': 7500000000000.00,\n",
    "        # 'vendor_risk_score': 0.85,\n",
    "        'receipt_quality': 0.65,\n",
    "        'ocr_confidence': 0.90,\n",
    "        # 'num_attendees': 3,\n",
    "        # 'employee_risk_score': 0.70,\n",
    "        'previous_violations': 1,\n",
    "        'department': 'Engineering',\n",
    "        # 'seniority' is omitted intentionally\n",
    "        'category': 'Travel',\n",
    "        'currency': 'INR',\n",
    "        'vendor_country': 'US',\n",
    "        'payment_method': 'Credit Card',\n",
    "        # 'cost_center': 'B2',\n",
    "        'expense_date': (datetime.today() - timedelta(days=5)).strftime('%Y-%m-%d'),\n",
    "        'submission_date': datetime.today().strftime('%Y-%m-%d'),\n",
    "        'requires_approval': 1,\n",
    "        # 'high_risk_category': 1,\n",
    "        'has_receipt': 1,\n",
    "        'manual_review_required': 0,\n",
    "        # Extra non-feature fields\n",
    "        'vendor_id': 2600,\n",
    "        'vendor_name': 'VendorA',\n",
    "        # 'project_code': 'P200',\n",
    "        'notes': '',\n",
    "        'approval_status': 'Approved',\n",
    "        'approval_date': datetime.today().strftime('%Y-%m-%d')\n",
    "    }\n",
    "    \n",
    "    report = detector.predict_single(new_expense)\n",
    "    print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
